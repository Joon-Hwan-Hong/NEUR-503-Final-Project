{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEUR 503 Hippocampally Dependent Navigation Model\n",
    "#### Hong, Joon Hwan (260832806)\n",
    "\n",
    "## Bookmarks\n",
    "### Section for:\n",
    "1) <a href=#1>Simulation class</a>\n",
    "\n",
    "2) <a href=#4>Experiments</a>\n",
    "\n",
    "### Note:\n",
    "due to the random initialization of mice position, the figures will look different every time it is ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import math\n",
    "from matplotlib.patches import Circle\n",
    "import mpl_toolkits.mplot3d.art3d as art3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermaze class (the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class watermaze(object):\n",
    "    \"\"\"\n",
    "    Defines a set of functions for simulating a rat moving in a water-maze.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_radius=60, platform_radius=10, platform_location=np.array([25,25]), stepsize=5.0, momentum=0.2, T=60):\n",
    "        \"\"\"\n",
    "        stepsize specifies how far the rat moves in one step.   \n",
    "        momentum is the ratio of old movement to new movement direction\n",
    "        (i.e. momentum = 0 means all new movement, momentum = 1 means all old movement).    \n",
    "        \"\"\"      \n",
    "        # *** constant vars ***\n",
    "        self.radius            = pool_radius\n",
    "        self.platform_radius   = platform_radius\n",
    "        self.platform_location = platform_location\n",
    "        self.stepsize          = stepsize\n",
    "        self.momentum          = momentum\n",
    "        self.T                 = T\n",
    "        \n",
    "        # *** changing vars ***\n",
    "        self.position = np.zeros((2,T))\n",
    "        self.t        = 0\n",
    "        self.prevdir  = np.zeros((2,))\n",
    "        \n",
    "        # for calculating directions\n",
    "        self.direction = {\n",
    "            0:  np.pi/2, # north\n",
    "            1:  np.pi/4, # north-east\n",
    "            2:  0, # east\n",
    "            3:  7*np.pi/4, # south-east\n",
    "            4:  3*np.pi/2, # south\n",
    "            5:  5*np.pi/4, # south-west\n",
    "            6:  np.pi, # west\n",
    "            7:  3*np.pi/4} # north-west\n",
    "        \n",
    "            \n",
    "    # **** RAT POSITION UPDATE ****\n",
    "    def move(self, A):\n",
    "        \"\"\"\n",
    "        Updates the model's position by moving it in the specified direction. \n",
    "        A is the selected direction given the direction dictionary definitions.\n",
    "        \"\"\"\n",
    "        # determine the vector of movement direction\n",
    "        angle = self.direction[A]\n",
    "        newdirection = np.array([np.cos(angle), np.sin(angle)])\n",
    "        \n",
    "        # add momentum to reflect \"actual swimming\" (and normalize, then multiply by stepsize)\n",
    "        direction = (1.0 - self.momentum)*newdirection + self.momentum*self.prevdir\n",
    "        direction = direction/np.sqrt((direction**2).sum())\n",
    "        direction = direction*self.stepsize\n",
    "        \n",
    "        # prevent the rat from actually leaving the water-maze: \"bounce\" off the wall\n",
    "        [newposition, direction] = self.poolreflect(self.position[:,self.t] + direction)\n",
    "        if (np.linalg.norm(newposition) == self.radius):\n",
    "            newposition = np.multiply(np.divide(newposition,np.linalg.norm(newposition)),(self.radius - 1))\n",
    "\n",
    "        # update position, time and previous direction\n",
    "        self.position[:,self.t+1] = newposition\n",
    "        self.t                    = self.t + 1\n",
    "        self.prevdir              = direction\n",
    "        \n",
    "            \n",
    "    # **** RAT BOUNCE IF AT EDGE ****\n",
    "    def poolreflect(self, newposition):\n",
    "        \"\"\"\n",
    "        returns the point in space at which the rat will be located if it bumps off the wall of the pool. \n",
    "        The function also returns the direction the rat will be headed.\n",
    "        \"\"\"\n",
    "        # determine if the newposition is outside the pool, if not, just return the new position\n",
    "        if (np.linalg.norm(newposition) < self.radius):\n",
    "            refposition  = newposition\n",
    "            refdirection = newposition - self.position[:,self.t]\n",
    "        else:\n",
    "            # determine where the rat hit the pool wall\n",
    "            px = self.intercept(newposition)\n",
    "            tx = np.asarray(np.matmul([[0, 1], [-1, 0]],px)) # the tangent vector; rotate -pi/2\n",
    "            dx = px - self.position[:,self.t] # vector of the direction of movement\n",
    "            \n",
    "            # angle between the direction of movement and the tangent vector\n",
    "            theta = np.arccos(np.matmul((np.divide(tx,np.linalg.norm(tx))).transpose(),(np.divide(dx,np.linalg.norm(dx))))).item()\n",
    "            \n",
    "            # get the reflected direction and position\n",
    "            ra = 2*(np.pi - theta)\n",
    "            refdirection = np.asarray(np.matmul([[np.cos(ra), -np.sin(ra)], [np.sin(ra), np.cos(ra)]],(newposition - px)))\n",
    "            refposition = px + refdirection\n",
    "            \n",
    "        return [refposition, refdirection]\n",
    "    \n",
    "            \n",
    "    def intercept(self,newposition):  \n",
    "        \"\"\"\n",
    "        returns the point in space at which the rat intercepts with the wall.\n",
    "        \"\"\"\n",
    "        p1 = self.position[:,self.t]\n",
    "        p2 = newposition\n",
    "\n",
    "        # calculate the terms used to find the point of intersection\n",
    "        dx = p2[0] - p1[0]\n",
    "        dy = p2[1] - p1[1]\n",
    "        dr = np.sqrt(np.power(dx,2) + np.power(dy,2))\n",
    "        D  = p1[0]*p2[1] - p2[0]*p1[1]\n",
    "        sy = np.sign(dy)\n",
    "        if (sy == 0):\n",
    "            sy = 1.0\n",
    "            \n",
    "        # calculate the potential points of intersection\n",
    "        pp1 = np.zeros((2,))\n",
    "        pp2 = np.zeros((2,))\n",
    "        pp1[0] = (D*dy + sy*dx*np.sqrt((np.power(self.radius,2))*(np.power(dr,2))-np.power(D,2)))/(np.power(dr,2))\n",
    "        pp2[0] = (D*dy - sy*dx*np.sqrt((np.power(self.radius,2))*(np.power(dr,2))-np.power(D,2)))/(np.power(dr,2))\n",
    "        pp1[1] = (-D*dx + np.absolute(dy)*np.sqrt((np.power(self.radius,2))*(np.power(dr,2))-np.power(D,2)))/(np.power(dr,2))\n",
    "        pp2[1] = (-D*dx - np.absolute(dy)*np.sqrt((np.power(self.radius,2))*(np.power(dr,2))-np.power(D,2)))/(np.power(dr,2))\n",
    "\n",
    "        # determine which intersection point is actually the right one (whichever is closer to p2)\n",
    "        px = pp1 if (np.linalg.norm(p2 - pp1) < np.linalg.norm(p2 - pp2)) else pp2\n",
    "\n",
    "        return px\n",
    "    \n",
    "            \n",
    "    # **** OTHER FUNCTIONS ****\n",
    "    def startposition(self):\n",
    "        '''\n",
    "        select a random location from the main cardinal axes and calculate it's vector angle\n",
    "        '''\n",
    "        condition = 2*np.random.randint(0,4)\n",
    "        angle = self.direction[condition]\n",
    "        self.position[:,0] = np.asarray([np.cos(angle), np.sin(angle)]) * (self.radius - 1)\n",
    "\n",
    "        \n",
    "    def timeup(self):\n",
    "        \"\"\"\n",
    "        returns true if the time for the trial is finished, false otherwise.\n",
    "        \"\"\" \n",
    "        return self.t > (self.T - 2)\n",
    "    \n",
    "            \n",
    "    def atgoal(self):\n",
    "        \"\"\"\n",
    "        returns true if the rat is on the platform, false otherwise.\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.sum((self.position[:,self.t] - self.platform_location)**2)) <= (self.platform_radius + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simulation Class <a name='1'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    '''\n",
    "    Designed a class to hold both the maze and algorithm, as well as the equations\n",
    "    necessary to run the simulation.\n",
    "    Algorithm should either be a string 'actor-critic' or 'combined'\n",
    "    '''\n",
    "    def __init__(self,algorithm, eta_a=0.1, eta_c=0.01, eta_xy=0.01, d_f=0.9):\n",
    "        # *** constant vars ***\n",
    "        self.N = 493 # number of place cells\n",
    "        self.sigma = 16 # gaussian width for place cells\n",
    "        self.Lambda = 0.9 # place cell history in coord model\n",
    "        \n",
    "        # *** changing vars ***\n",
    "        self.goal_xy = [] # goal coordinates\n",
    "        self.a_c = 0 # preferred action coord\n",
    "        \n",
    "        # *** identifiers ***\n",
    "        self.maze = None\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "        # *** weights ***\n",
    "        self.w_c = np.zeros(self.N) # critic weights\n",
    "        self.w_a = np.zeros([8, self.N]) # actor weights\n",
    "        self.w_x = np.zeros(self.N) # x-coord weights\n",
    "        self.w_y = np.zeros(self.N) # y-coord weights\n",
    "        \n",
    "        # *** learning ***\n",
    "        self.eta_a = eta_a # actor learning rate\n",
    "        self.eta_c = eta_c # critic learning rate\n",
    "        self.eta_xy = eta_xy # coordinate learning rate\n",
    "        self.d_f = d_f # discount factor\n",
    "        \n",
    "        # *** place cell coordinate generation ***\n",
    "        self.s = np.zeros([self.N,2])\n",
    "        self.create_space()\n",
    "        \n",
    "        \n",
    "    # SHOULD BE A PRIVATE FUNCTION BUT ITS PYTHON SO NOT GONNA BOTHER WITH THAT\n",
    "    def create_space(self,r=60):\n",
    "        '''\n",
    "        this function is to create the place cell vectors s_i as needed in the paper.\n",
    "        '''\n",
    "        # generate angle and rad\n",
    "        a = 2*np.pi*np.random.sample(self.N)\n",
    "        rad = r*np.sqrt(np.random.sample(self.N))\n",
    "        # create coordinates\n",
    "        self.s[:,0] = rad*np.cos(a)\n",
    "        self.s[:,1] = rad*np.sin(a)\n",
    "        \n",
    "    \n",
    "    # **** actor critic policy function ****     \n",
    "    def activity(self, p):\n",
    "        '''\n",
    "        calculates equation (1) in the paper, given the position p.\n",
    "        '''\n",
    "        return np.exp(-(np.linalg.norm(p-self.s, axis=1)**2)/(2*(self.sigma**2)))\n",
    "    \n",
    "    \n",
    "    # **** AC POLICY AND STEP ****\n",
    "    def policy(self, a_j):\n",
    "        '''\n",
    "        calculates equation (9) in the paper.\n",
    "        a_j is the eight possible directions (eight action cells j = 1...8)\n",
    "        and is stochastically chosen with probabilities P_j calculated.\n",
    "        '''\n",
    "        p_j = np.exp(2*a_j)/np.sum(np.exp(2*a_j))\n",
    "        return np.random.choice(len(p_j), p=p_j)\n",
    "    \n",
    "    \n",
    "    def step(self, A, t):\n",
    "        '''\n",
    "        a function for timestep.\n",
    "        moves in the maze, then gets the new position, then calculates reward.\n",
    "        Return a tuple with (new position, reward), where reward is a boolean 0 or 1.\n",
    "        '''\n",
    "        self.maze.move(A)\n",
    "        p_new = self.maze.position[:,t+1] \n",
    "        return (p_new, 1) if self.maze.atgoal() else (p_new, 0)\n",
    "\n",
    "    \n",
    "    # **** OUT FUNCTIONS ****    \n",
    "    def out_actor(self, p):\n",
    "        '''\n",
    "        calculates actor output as stated in the paper. summation across i is equiv to a dot product.\n",
    "        '''\n",
    "        return np.dot(self.w_a, self.activity(p))\n",
    "    \n",
    "    \n",
    "    def out_critic(self, p):\n",
    "        '''\n",
    "        calculates equation (2) in the paper. summation across i is equiv to a dot product.\n",
    "        '''\n",
    "        return np.dot(self.w_c, self.activity(p))\n",
    "    \n",
    "    \n",
    "    def out_x(self, p):\n",
    "        '''\n",
    "        x estimate coordinate.\n",
    "        '''\n",
    "        return np.dot(self.w_x, self.activity(p))\n",
    "        \n",
    "        \n",
    "    def out_y(self, p):\n",
    "        '''\n",
    "        y estimate coordinate.\n",
    "        '''\n",
    "        return np.dot(self.w_y, self.activity(p))\n",
    "    \n",
    "    \n",
    "    # **** UPDATE FUNCTIONS ****\n",
    "    def update_a(self, delta, p, A):\n",
    "        '''\n",
    "        calculate equation (10) in the paper. update by incrimentation. \n",
    "        delta = temporal difference error.\n",
    "        '''\n",
    "        g = np.zeros(8)\n",
    "        g[A] = 1\n",
    "        self.w_a += self.eta_a*delta*np.outer(g, self.activity(p))\n",
    "        \n",
    "        \n",
    "    def update_c(self, delta, p):\n",
    "        '''\n",
    "        calculate equation (8) in the paper. update by incrimentation.\n",
    "        '''\n",
    "        self.w_c += self.eta_c*delta*self.activity(p)\n",
    "        \n",
    "        \n",
    "    def update_a_c(self, delta, const=75):\n",
    "        '''\n",
    "        update preferred action coordinate.\n",
    "        '''\n",
    "        self.a_c += const*self.eta_a*delta\n",
    "        \n",
    "        \n",
    "    def update_history(self, hist, p):\n",
    "        '''\n",
    "        update history of activations.\n",
    "        '''\n",
    "        return ((hist*self.Lambda) + self.activity(p))\n",
    "    \n",
    "    \n",
    "    def update_coords(self, p_t, p_new, hist):\n",
    "        '''\n",
    "        calculates equation (11) and (12) in the paper.\n",
    "        '''\n",
    "        self.w_x += hist*self.eta_xy*(p_t[0]-p_new[0]+self.out_x(p_new)-self.out_x(p_t))\n",
    "        self.w_y += hist*self.eta_xy*(p_t[1]-p_new[1]+self.out_y(p_new)-self.out_y(p_t))\n",
    "   \n",
    "\n",
    "    # **** COORDINATE MODEL FUNCTIONS ****\n",
    "    def optimal_action(self, x1, x2=np.array([0, 1])):\n",
    "        '''\n",
    "        calculate optimal action given a direction vector x1 in the coordinate model.\n",
    "        '''\n",
    "        theta = math.atan2(np.linalg.det([x1, x2]), np.dot(x1, x2))\n",
    "        theta = theta if (theta > 0) else theta + 2*np.pi # workaround\n",
    "        # reminder to self: action is an int ranging from 0 to 8, and 0&8 are equiv\n",
    "        return int(round(theta/(np.pi/4)))%8\n",
    "    \n",
    "    \n",
    "    def step_coord(self,p):\n",
    "        '''\n",
    "        if a goal position is known, move towards it, else random.\n",
    "        '''\n",
    "        if len(self.goal_xy) is not 0:\n",
    "            x1 = np.subtract(self.goal_xy,[self.out_x(p), self.out_y(p)])\n",
    "            ans = self.optimal_action(x1)\n",
    "        else:\n",
    "            ans = np.random.randint(8)\n",
    "        return ans\n",
    "    \n",
    "    \n",
    "    # **** SIMULATION FUNCTION ****\n",
    "    def run_simulation(self):\n",
    "        '''\n",
    "        Both follow the same pattern of:\n",
    "        1) a_j --> A --> p_new and reward\n",
    "        2) calculate TD error\n",
    "        3) update model based on TD error and other parameters\n",
    "        ''' \n",
    "        # initialize\n",
    "        t = 0\n",
    "        dist = 0\n",
    "        self.maze.startposition()\n",
    "        p_t = self.maze.position[:,t]\n",
    "        \n",
    "        # *** ACTOR CRITIC MODE ***\n",
    "        if self.algorithm == 'actor-critic':\n",
    "            while not (self.maze.atgoal() or self.maze.timeup()):\n",
    "                # get a_j, A, and from that, p_new and the reward from timestep.\n",
    "                a_j = self.out_actor(p_t)\n",
    "                A = self.policy(a_j)\n",
    "                p_new, reward = self.step(A, t)\n",
    "                \n",
    "                # calculate TD error\n",
    "                TD_error = reward - self.out_critic(p_t) if (reward == 1) else (self.d_f * self.out_critic(p_new))-self.out_critic(p_t)\n",
    "                \n",
    "                # update actor and critic weights, distance, position, and time\n",
    "                self.update_a(TD_error, p_t, A)\n",
    "                self.update_c(TD_error, p_t)\n",
    "                dist += np.linalg.norm(p_new-p_t)\n",
    "                p_t = p_new\n",
    "                t += 1\n",
    "            return (t, dist)\n",
    "        \n",
    "        # *** COMBINED MODE ***\n",
    "        if self.algorithm == 'combined':\n",
    "            history = np.zeros(self.N)\n",
    "            while not (self.maze.atgoal() or self.maze.timeup()):\n",
    "                flag_a = False\n",
    "                # append the preferred action/coordinate to a_j. Timestep. \n",
    "                a_j = np.append(self.out_actor(p_t), self.a_c)\n",
    "                A = self.policy(a_j)\n",
    "                # calculate coordinate model step if selected\n",
    "                if A == 8:\n",
    "                    A = self.step_coord(p_t)\n",
    "                    flag_a = True\n",
    "                p_new, reward = self.step(A, t)\n",
    "                \n",
    "                # calculate TD error\n",
    "                TD_error = reward - self.out_critic(p_t) if (reward == 1) else (self.d_f * self.out_critic(p_new))-self.out_critic(p_t)\n",
    "                \n",
    "                # update history\n",
    "                history = self.update_history(history, p_t)\n",
    "                self.update_c(TD_error, p_t)\n",
    "                self.update_coords(p_t, p_new, history) \n",
    "                # update coordinate & actor weights depending on situation & others\n",
    "                if (flag_a and (len(self.goal_xy) != 0)):\n",
    "                    self.update_a_c(TD_error)\n",
    "                else:\n",
    "                    self.update_a(TD_error, p_t, A)\n",
    "                dist += np.linalg.norm(p_new-p_t)\n",
    "                p_t = p_new\n",
    "                t += 1\n",
    "                # update goal xy if necessary\n",
    "                self.goal_xy = [self.out_x(p_t), self.out_y(p_t)] if (reward == 1) else []\n",
    "            return (t, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments <a name='4'/>\n",
    "\n",
    "\n",
    "### Multi-Platform Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP_flag = True # flag for single or multi-platform cases\n",
    "\n",
    "# locations are arbitrary for debug purposes\n",
    "#platform_coords = random.randint(-60, 60, size=(5,2))\n",
    "platform_coords = [[15,15], [-15,-15], [30,10], [-10, 30], [-40,20]]\n",
    "platforms = np.array(platform_coords) if MP_flag else np.array(platform_coords[0])\n",
    "n_p = len(platforms) if MP_flag else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTOR-CRITIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** initialize variables ***\n",
    "n_t = 50 # number of trials\n",
    "runs = 10 if MP_flag else 50\n",
    "dim_ini = [runs, n_p*n_t] # initialized dimension of data\n",
    "data_t = np.zeros(dim_ini) # time\n",
    "data_d = np.zeros(dim_ini) # distance\n",
    "algorithm = 'actor-critic' # 'actor-critic' or 'combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get data from each timestep\n",
    "n_runs = np.arange(runs)\n",
    "for run in n_runs:\n",
    "    # the model with the selected algorithm\n",
    "    model = Simulation(algorithm)\n",
    "    for i in range(n_p):\n",
    "        for j in range(n_t):\n",
    "            i_maze = watermaze(platform_location=platforms[i])\n",
    "            model.maze = i_maze\n",
    "            time, dist = model.run_simulation()\n",
    "            data_t[run, (i*n_t)+j] = time\n",
    "            data_d[run, (i*n_t)+j] = dist\n",
    "            \n",
    "# mean calculations, time data can be used as well just by replacing the end tag char from d to t's.pain \n",
    "mean_t = np.mean(data_t, axis=0)\n",
    "mean_d = np.mean(data_d, axis=0)\n",
    "std_t = np.std(data_t, axis=0)\n",
    "std_d = np.std(data_d, axis=0)\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(16,9)) # screen aspect ratio\n",
    "\n",
    "if MP_flag:\n",
    "    des_ranges = [[0, 49],[50, 99],[100, 149],[150, 199],[200, 249]] # note this would change if n_t changes\n",
    "    for ran in des_ranges:\n",
    "        xPartition = np.arange(ran[0], ran[1])\n",
    "        yPartition = mean_d[ran[0]: ran[1]]\n",
    "        plt.errorbar(xPartition, yPartition, std_d[ran[0]: ran[1]]/2)\n",
    "        plt.plot(xPartition,yPartition)\n",
    "else:\n",
    "    x_vals = np.arange(len(mean_d))\n",
    "    plt.errorbar(x_vals, mean_d, std_d/2)\n",
    "    plt.plot(x_vals, mean_d)\n",
    "\n",
    "plt.title(f'Path length using Actor-Critic ({n_t} trials per)')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Path length (cm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** initialize variables ***\n",
    "n_t = 50 # number of trials\n",
    "runs = 10 if MP_flag else 50\n",
    "dim_ini = [runs, n_p*n_t] # initialized dimension of data\n",
    "data_t = np.zeros(dim_ini) # time\n",
    "data_d = np.zeros(dim_ini) # distance\n",
    "algorithm = 'combined' # 'actor-critic' or 'combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from each timestep\n",
    "n_runs = np.arange(runs)\n",
    "for run in n_runs:\n",
    "    # the model with the selected algorithm\n",
    "    model = Simulation(algorithm, d_f=0.80)\n",
    "    for i in range(n_p):\n",
    "        for j in range(n_t):\n",
    "            i_maze = watermaze(platform_location=platforms[i])\n",
    "            model.maze = i_maze\n",
    "            time, dist = model.run_simulation()\n",
    "            data_t[run, (i*n_t)+j] = time\n",
    "            data_d[run, (i*n_t)+j] = dist\n",
    "            \n",
    "# mean calculations\n",
    "mean_t = np.mean(data_t, axis=0)\n",
    "mean_d = np.mean(data_d, axis=0)\n",
    "std_t = np.std(data_t, axis=0)\n",
    "std_d = np.std(data_d, axis=0)\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "if MP_flag:\n",
    "    des_ranges = [[0, 49],[50, 99],[100, 149],[150, 199],[200, 249]] # would change according to n_t\n",
    "    for ran in des_ranges:\n",
    "        xPartition = np.arange(ran[0], ran[1])\n",
    "        yPartition = mean_d[ran[0]: ran[1]]\n",
    "        plt.errorbar(xPartition, yPartition, std_d[ran[0]: ran[1]]/2)\n",
    "        plt.plot(xPartition,yPartition)\n",
    "else:\n",
    "    x_vals = np.arange(len(mean_d))\n",
    "    plt.errorbar(x_vals, mean_d, std_d/2)\n",
    "    plt.plot(x_vals, mean_d)\n",
    "\n",
    "plt.title(f'Path length using Combined ({n_t} trials per)')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Path length (cm)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
